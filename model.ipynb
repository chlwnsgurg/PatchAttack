{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 코드는 전이학습을 이용하여 GTSRB(독일교통표지판) 데이터 분류를 위한 모델을 만드는 과정과 결과를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "# %pip install\n",
    "\n",
    "# Create a directory to store our results\n",
    "# ! mkdir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights,ResNet34_Weights,ResNet50_Weights,ResNet101_Weights,ResNet152_Weights\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GTRSB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "import torchvision.transforms as transforms\n",
    "preprocess = transforms.Compose(\n",
    "    [transforms.Resize([112, 112]),\n",
    "    transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# Load Data\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder(root=\"./GTSRB/Train\", transform=preprocess)\n",
    "\n",
    "# Divide data into training and validation (0.8 and 0.2)\n",
    "ratio = 0.8\n",
    "n_train_examples = int(len(train_set) * ratio)\n",
    "n_val_examples = len(train_set) - n_train_examples\n",
    "gtsrb_train_data, val_data = data.random_split(train_set, [n_train_examples, n_val_examples])\n",
    "train_loader = data.DataLoader(gtsrb_train_data, shuffle=True, batch_size = BATCH_SIZE)\n",
    "val_loader = data.DataLoader(val_data, shuffle=True, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 pre-trained ResNet-18으로 초기화한 후 finetuning을 진행하였다.\n",
    "해당 모델은 약 99.9%의 분류 정확도를 보였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# or any of these variants\n",
    "# model = models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "# model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "# model = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "in_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_ftrs, 43) # GTSRB dataset consists of 39209 training images corresponding to 43 classes.\n",
    "\n",
    "# move the model to GPU for speed if available\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./Traffic_Sign_Classification.pth\n",
      "\n",
      "Epoch-0:\n",
      "Training: Loss = 25.9320, Accuracy = 0.9424, Time = 213.76 seconds\n",
      "Validation: Loss = 1.3289, Accuracy = 0.9883, Time = 15.74 seconds\n",
      "Epoch-1:\n",
      "Training: Loss = 1.9956, Accuracy = 0.9957, Time = 218.25 seconds\n",
      "Validation: Loss = 1.7674, Accuracy = 0.9841, Time = 16.03 seconds\n",
      "Epoch-2:\n",
      "Training: Loss = 2.7767, Accuracy = 0.9945, Time = 213.13 seconds\n",
      "Validation: Loss = 1.3302, Accuracy = 0.9871, Time = 15.60 seconds\n",
      "Epoch-3:\n",
      "Training: Loss = 1.0924, Accuracy = 0.9978, Time = 214.36 seconds\n",
      "Validation: Loss = 0.3212, Accuracy = 0.9972, Time = 15.85 seconds\n",
      "Epoch-4:\n",
      "Training: Loss = 0.2918, Accuracy = 0.9995, Time = 223.51 seconds\n",
      "Validation: Loss = 0.2614, Accuracy = 0.9978, Time = 15.82 seconds\n",
      "Epoch-5:\n",
      "Training: Loss = 0.3376, Accuracy = 0.9992, Time = 215.23 seconds\n",
      "Validation: Loss = 0.1880, Accuracy = 0.9983, Time = 16.56 seconds\n",
      "Epoch-6:\n",
      "Training: Loss = 2.2706, Accuracy = 0.9954, Time = 419.85 seconds\n",
      "Validation: Loss = 0.5117, Accuracy = 0.9946, Time = 17.95 seconds\n",
      "Epoch-7:\n",
      "Training: Loss = 1.2131, Accuracy = 0.9973, Time = 291.37 seconds\n",
      "Validation: Loss = 0.1134, Accuracy = 0.9994, Time = 17.59 seconds\n",
      "Epoch-8:\n",
      "Training: Loss = 0.0775, Accuracy = 0.9999, Time = 286.76 seconds\n",
      "Validation: Loss = 0.2231, Accuracy = 0.9982, Time = 18.00 seconds\n",
      "Epoch-9:\n",
      "Training: Loss = 1.0358, Accuracy = 0.9977, Time = 256.44 seconds\n",
      "Validation: Loss = 1.2045, Accuracy = 0.9908, Time = 15.51 seconds\n",
      "Epoch-10:\n",
      "Training: Loss = 1.0117, Accuracy = 0.9982, Time = 214.21 seconds\n",
      "Validation: Loss = 0.4445, Accuracy = 0.9958, Time = 15.48 seconds\n",
      "Epoch-11:\n",
      "Training: Loss = 0.4132, Accuracy = 0.9991, Time = 254.93 seconds\n",
      "Validation: Loss = 0.1942, Accuracy = 0.9978, Time = 15.86 seconds\n",
      "Epoch-12:\n",
      "Training: Loss = 0.5490, Accuracy = 0.9988, Time = 229.00 seconds\n",
      "Validation: Loss = 1.8810, Accuracy = 0.9832, Time = 17.00 seconds\n",
      "Epoch-13:\n",
      "Training: Loss = 1.3752, Accuracy = 0.9971, Time = 255.30 seconds\n",
      "Validation: Loss = 0.2227, Accuracy = 0.9981, Time = 18.03 seconds\n",
      "Epoch-14:\n",
      "Training: Loss = 1.4577, Accuracy = 0.9972, Time = 235.71 seconds\n",
      "Validation: Loss = 0.1063, Accuracy = 0.9992, Time = 15.56 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define a Loss function and optimizer\n",
    "LR = 0.001\n",
    "EPOCHS = 15\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Path to save model\n",
    "PATH_TO_MODEL = \"./Traffic_Sign_Classification.pth\"\n",
    "best_acc = 0.0\n",
    "\n",
    "print(\"Model saved at \"+PATH_TO_MODEL)\n",
    "print('')\n",
    "\n",
    "# Perform training\n",
    "train_loss_list = [0]*EPOCHS\n",
    "train_acc_list = [0]*EPOCHS\n",
    "val_loss_list = [0]*EPOCHS\n",
    "val_acc_list = [0]*EPOCHS\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch-{epoch}:\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_start_time = time.monotonic()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct,epoch_total=0,0\n",
    "    for (images, labels) in train_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        top_preds = outputs.argmax(1, keepdim = True)\n",
    "        correct = top_preds.eq(labels.view_as(top_preds)).sum()\n",
    "\n",
    "        # Optimizing weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct.item()\n",
    "        epoch_total += labels.shape[0]\n",
    "    train_end_time = time.monotonic()\n",
    "    train_loss = epoch_loss\n",
    "    train_acc = epoch_correct/epoch_total\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_start_time = time.monotonic()\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct,epoch_total=0,0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in val_loader:\n",
    "            images=images.to(device)\n",
    "            labels=labels.to(device)\n",
    "\n",
    "            # Run predictions\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            top_preds = outputs.argmax(1, keepdim = True)\n",
    "            correct = top_preds.eq(labels.view_as(top_preds)).sum()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_correct += correct.item()\n",
    "            epoch_total += labels.shape[0]\n",
    "    val_end_time = time.monotonic()\n",
    "    val_loss = epoch_loss\n",
    "    val_acc = epoch_correct/epoch_total\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), PATH_TO_MODEL)\n",
    "\n",
    "    train_loss_list[epoch] = train_loss\n",
    "    train_acc_list[epoch] = train_acc\n",
    "    val_loss_list[epoch] = val_loss\n",
    "    val_acc_list[epoch] = val_acc\n",
    "    \n",
    "    print(\"Training: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (train_loss, train_acc, train_end_time - train_start_time))\n",
    "    print(\"Validation: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (val_loss, val_acc, val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy: 99.93624114990234\n"
     ]
    }
   ],
   "source": [
    "# val model accuracy\n",
    "model.load_state_dict(torch.load(PATH_TO_MODEL))\n",
    "correct,total=0,0\n",
    "with torch.no_grad():\n",
    "    for images,labels in val_loader:\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1, keepdim = True)\n",
    "        correct += preds.eq(labels.view_as(preds)).sum()\n",
    "        total += labels.shape[0]\n",
    "print(f\"model accuracy: {100 * correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
